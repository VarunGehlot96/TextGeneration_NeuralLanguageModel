{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465f79fc-6ba6-4e00-8dc5-c00af8158b34",
   "metadata": {},
   "source": [
    "# Project By Varun Gehlot\n",
    "\n",
    "## Objective:\n",
    "### Text Generation Project to predict the next word(s). \n",
    "\n",
    "## Corpus:\n",
    "### The Republic by Plato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c76255-c24d-4294-9a3d-9cf975429f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what \n"
     ]
    }
   ],
   "source": [
    "# loading the file\n",
    "\n",
    "def load_doc(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "doc = load_doc('republic_clean.txt')\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b8c241-3e72-4de5-8211-8dbd89b1b13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 117584\n",
      "Unique tokens: 7234\n"
     ]
    }
   ],
   "source": [
    "# cleaning the file\n",
    "\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('-',' ')\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile(f\"[{re.escape(punctuation)}]\")\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = clean_doc(doc)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ebe01d-231c-4da5-afa2-c6043aa25911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117533\n"
     ]
    }
   ],
   "source": [
    "# organizing into sequence of tokens and saving it into a file (training data)\n",
    "\n",
    "length = 50 + 1\n",
    "sequences = []\n",
    "\n",
    "for i in range(length,len(tokens)):\n",
    "    seq = tokens[i-length : i] # slecting sequence of tokens\n",
    "    line = ' '.join(seq)  # converting into a line\n",
    "    sequences.append(line) # adding the sequence to the list \n",
    "\n",
    "print(len(sequences))\n",
    "\n",
    "def save_doc(sequences,filename):\n",
    "    data = '\\n'.join(sequences)\n",
    "    with open(filename,'w') as file:\n",
    "        text = file.write(data)\n",
    "\n",
    "save_doc(sequences = sequences, filename = 'republic_sequences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd705cb9-0a35-49f6-a0f4-3d070082d012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fitting a language model to this data\n",
    "\n",
    "doc = load_doc('republic_sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921ea6c7-2eac-4135-83d4-529ddb945314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "117533\n"
     ]
    }
   ],
   "source": [
    "# integer encoding sequences of words \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines) # so that each word is encoded with unique integers\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a278b9-50bb-4d7d-a83a-b655a51f072b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7235"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size (for defining the embedding layer) # input_dim\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb6813a-3f8b-4e3b-b19f-2913716a8e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separating input and output\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical # for one hot encoding (grond truth for model to refer and learn)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X,y = sequences[:,:-1] , sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size) # num_classes, number of unique categories\n",
    "seq_length = X.shape[1] # 50, learned embedding needs to know vocab_size and length of input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98d2ae0-dcf9-449a-bf90-8f10b2eb0adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "def define_model(vocab_size, seq_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length = seq_length)) # 50 is output_dim (vector space)\n",
    "    model.add(LSTM(100, return_sequences = True)) # returns full sequence (often used when we use another layer lstm layer next)\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "    # configure network\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987c8242-8e55-4988-90fa-5f22c6784ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            361750    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7235)              730735    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1243385 (4.74 MB)\n",
      "Trainable params: 1243385 (4.74 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/125\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "919/919 [==============================] - 52s 54ms/step - loss: 6.1363 - accuracy: 0.0742\n",
      "Epoch 2/125\n",
      "919/919 [==============================] - 51s 55ms/step - loss: 5.6726 - accuracy: 0.1082\n",
      "Epoch 3/125\n",
      "919/919 [==============================] - 52s 56ms/step - loss: 5.4377 - accuracy: 0.1315\n",
      "Epoch 4/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 5.2884 - accuracy: 0.1455\n",
      "Epoch 5/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 5.1797 - accuracy: 0.1534\n",
      "Epoch 6/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 5.0829 - accuracy: 0.1596\n",
      "Epoch 7/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 4.9949 - accuracy: 0.1662\n",
      "Epoch 8/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 4.9122 - accuracy: 0.1716\n",
      "Epoch 9/125\n",
      "919/919 [==============================] - 55s 59ms/step - loss: 4.8331 - accuracy: 0.1758\n",
      "Epoch 10/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 4.7604 - accuracy: 0.1791\n",
      "Epoch 11/125\n",
      "919/919 [==============================] - 53s 58ms/step - loss: 4.6915 - accuracy: 0.1826\n",
      "Epoch 12/125\n",
      "919/919 [==============================] - 53s 58ms/step - loss: 4.8185 - accuracy: 0.1708\n",
      "Epoch 13/125\n",
      "919/919 [==============================] - 53s 57ms/step - loss: 4.6496 - accuracy: 0.1824\n",
      "Epoch 14/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 4.5705 - accuracy: 0.1879\n",
      "Epoch 15/125\n",
      "919/919 [==============================] - 56s 61ms/step - loss: 4.5038 - accuracy: 0.1909\n",
      "Epoch 16/125\n",
      "919/919 [==============================] - 59s 64ms/step - loss: 4.4372 - accuracy: 0.1947\n",
      "Epoch 17/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 4.3739 - accuracy: 0.1988\n",
      "Epoch 18/125\n",
      "919/919 [==============================] - 53s 58ms/step - loss: 4.3128 - accuracy: 0.2015\n",
      "Epoch 19/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 4.2504 - accuracy: 0.2044\n",
      "Epoch 20/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 4.1907 - accuracy: 0.2081\n",
      "Epoch 21/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 4.1321 - accuracy: 0.2117\n",
      "Epoch 22/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 4.0750 - accuracy: 0.2156\n",
      "Epoch 23/125\n",
      "919/919 [==============================] - 53s 58ms/step - loss: 4.0188 - accuracy: 0.2188\n",
      "Epoch 24/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.9639 - accuracy: 0.2231\n",
      "Epoch 25/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 3.9127 - accuracy: 0.2269\n",
      "Epoch 26/125\n",
      "919/919 [==============================] - 53s 58ms/step - loss: 3.8622 - accuracy: 0.2320\n",
      "Epoch 27/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.8113 - accuracy: 0.2366\n",
      "Epoch 28/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 3.7632 - accuracy: 0.2404\n",
      "Epoch 29/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.7159 - accuracy: 0.2454\n",
      "Epoch 30/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.6697 - accuracy: 0.2502\n",
      "Epoch 31/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 3.6242 - accuracy: 0.2536\n",
      "Epoch 32/125\n",
      "919/919 [==============================] - 54s 58ms/step - loss: 3.5821 - accuracy: 0.2589\n",
      "Epoch 33/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.5395 - accuracy: 0.2650\n",
      "Epoch 34/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.4978 - accuracy: 0.2688\n",
      "Epoch 35/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.4618 - accuracy: 0.2743\n",
      "Epoch 36/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.4152 - accuracy: 0.2788\n",
      "Epoch 37/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.3754 - accuracy: 0.2849\n",
      "Epoch 38/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.3485 - accuracy: 0.2901\n",
      "Epoch 39/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.9655 - accuracy: 0.2409\n",
      "Epoch 40/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 3.8219 - accuracy: 0.2443\n",
      "Epoch 41/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.6213 - accuracy: 0.2632\n",
      "Epoch 42/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.9336 - accuracy: 0.2316\n",
      "Epoch 43/125\n",
      "919/919 [==============================] - 54s 59ms/step - loss: 4.0223 - accuracy: 0.2170\n",
      "Epoch 44/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.9938 - accuracy: 0.2170\n",
      "Epoch 45/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.9773 - accuracy: 0.2155\n",
      "Epoch 46/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.9048 - accuracy: 0.2224\n",
      "Epoch 47/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.8461 - accuracy: 0.2279\n",
      "Epoch 48/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.7951 - accuracy: 0.2333\n",
      "Epoch 49/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.7456 - accuracy: 0.2375\n",
      "Epoch 50/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.7014 - accuracy: 0.2430\n",
      "Epoch 51/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.6586 - accuracy: 0.2487\n",
      "Epoch 52/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.6201 - accuracy: 0.2523\n",
      "Epoch 53/125\n",
      "919/919 [==============================] - 56s 61ms/step - loss: 3.5789 - accuracy: 0.2581\n",
      "Epoch 54/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.5409 - accuracy: 0.2621\n",
      "Epoch 55/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.5037 - accuracy: 0.2667\n",
      "Epoch 56/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.4692 - accuracy: 0.2711\n",
      "Epoch 57/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.4337 - accuracy: 0.2746\n",
      "Epoch 58/125\n",
      "919/919 [==============================] - 56s 60ms/step - loss: 3.4002 - accuracy: 0.2820\n",
      "Epoch 59/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.3706 - accuracy: 0.2846\n",
      "Epoch 60/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.3288 - accuracy: 0.2907\n",
      "Epoch 61/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.2998 - accuracy: 0.2930\n",
      "Epoch 62/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.2675 - accuracy: 0.2992\n",
      "Epoch 63/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.2393 - accuracy: 0.3023\n",
      "Epoch 64/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.2110 - accuracy: 0.3071\n",
      "Epoch 65/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.1778 - accuracy: 0.3109\n",
      "Epoch 66/125\n",
      "919/919 [==============================] - 56s 60ms/step - loss: 3.1510 - accuracy: 0.3154\n",
      "Epoch 67/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.1202 - accuracy: 0.3210\n",
      "Epoch 68/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.0889 - accuracy: 0.3254\n",
      "Epoch 69/125\n",
      "919/919 [==============================] - 56s 61ms/step - loss: 3.0592 - accuracy: 0.3293\n",
      "Epoch 70/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 3.0299 - accuracy: 0.3351\n",
      "Epoch 71/125\n",
      "919/919 [==============================] - 56s 60ms/step - loss: 3.0027 - accuracy: 0.3388\n",
      "Epoch 72/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.9719 - accuracy: 0.3438\n",
      "Epoch 73/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.9417 - accuracy: 0.3495\n",
      "Epoch 74/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.9148 - accuracy: 0.3529\n",
      "Epoch 75/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.8887 - accuracy: 0.3580\n",
      "Epoch 76/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.8594 - accuracy: 0.3633\n",
      "Epoch 77/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.8314 - accuracy: 0.3669\n",
      "Epoch 78/125\n",
      "919/919 [==============================] - 55s 60ms/step - loss: 2.8035 - accuracy: 0.3719\n",
      "Epoch 79/125\n",
      "919/919 [==============================] - 56s 60ms/step - loss: 2.7762 - accuracy: 0.3764\n",
      "Epoch 80/125\n",
      "919/919 [==============================] - 67s 73ms/step - loss: 2.7477 - accuracy: 0.3813\n",
      "Epoch 81/125\n",
      "919/919 [==============================] - 195s 213ms/step - loss: 2.7247 - accuracy: 0.3855\n",
      "Epoch 82/125\n",
      "919/919 [==============================] - 194s 211ms/step - loss: 2.6968 - accuracy: 0.3897\n",
      "Epoch 83/125\n",
      "919/919 [==============================] - 192s 209ms/step - loss: 2.6684 - accuracy: 0.3957\n",
      "Epoch 84/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 2.6435 - accuracy: 0.3989\n",
      "Epoch 85/125\n",
      "919/919 [==============================] - 196s 213ms/step - loss: 2.6177 - accuracy: 0.4045\n",
      "Epoch 86/125\n",
      "919/919 [==============================] - 194s 212ms/step - loss: 2.5916 - accuracy: 0.4091\n",
      "Epoch 87/125\n",
      "919/919 [==============================] - 193s 210ms/step - loss: 2.5669 - accuracy: 0.4144\n",
      "Epoch 88/125\n",
      "919/919 [==============================] - 194s 211ms/step - loss: 2.5418 - accuracy: 0.4185\n",
      "Epoch 89/125\n",
      "919/919 [==============================] - 196s 213ms/step - loss: 2.5155 - accuracy: 0.4233\n",
      "Epoch 90/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 2.4909 - accuracy: 0.4259\n",
      "Epoch 91/125\n",
      "919/919 [==============================] - 196s 213ms/step - loss: 2.4671 - accuracy: 0.4314\n",
      "Epoch 92/125\n",
      "919/919 [==============================] - 191s 208ms/step - loss: 2.4450 - accuracy: 0.4358\n",
      "Epoch 93/125\n",
      "919/919 [==============================] - 193s 210ms/step - loss: 2.4193 - accuracy: 0.4398\n",
      "Epoch 94/125\n",
      "919/919 [==============================] - 194s 211ms/step - loss: 2.3965 - accuracy: 0.4434\n",
      "Epoch 95/125\n",
      "919/919 [==============================] - 194s 211ms/step - loss: 2.3717 - accuracy: 0.4496\n",
      "Epoch 96/125\n",
      "919/919 [==============================] - 194s 211ms/step - loss: 2.3460 - accuracy: 0.4546\n",
      "Epoch 97/125\n",
      "919/919 [==============================] - 191s 208ms/step - loss: 2.3277 - accuracy: 0.4585\n",
      "Epoch 98/125\n",
      "919/919 [==============================] - 196s 213ms/step - loss: 2.3028 - accuracy: 0.4633\n",
      "Epoch 99/125\n",
      "919/919 [==============================] - 184s 200ms/step - loss: 2.2820 - accuracy: 0.4666\n",
      "Epoch 100/125\n",
      "919/919 [==============================] - 184s 200ms/step - loss: 2.2621 - accuracy: 0.4711\n",
      "Epoch 101/125\n",
      "919/919 [==============================] - 190s 207ms/step - loss: 2.2377 - accuracy: 0.4765\n",
      "Epoch 102/125\n",
      "919/919 [==============================] - 191s 207ms/step - loss: 2.2146 - accuracy: 0.4807\n",
      "Epoch 103/125\n",
      "919/919 [==============================] - 186s 203ms/step - loss: 2.1969 - accuracy: 0.4831\n",
      "Epoch 104/125\n",
      "919/919 [==============================] - 191s 208ms/step - loss: 2.1720 - accuracy: 0.4887\n",
      "Epoch 105/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 2.1503 - accuracy: 0.4938\n",
      "Epoch 106/125\n",
      "919/919 [==============================] - 194s 211ms/step - loss: 2.1341 - accuracy: 0.4954\n",
      "Epoch 107/125\n",
      "919/919 [==============================] - 192s 209ms/step - loss: 2.1114 - accuracy: 0.5007\n",
      "Epoch 108/125\n",
      "919/919 [==============================] - 191s 208ms/step - loss: 2.0889 - accuracy: 0.5047\n",
      "Epoch 109/125\n",
      "919/919 [==============================] - 193s 211ms/step - loss: 2.0729 - accuracy: 0.5076\n",
      "Epoch 110/125\n",
      "919/919 [==============================] - 190s 207ms/step - loss: 2.0503 - accuracy: 0.5121\n",
      "Epoch 111/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 2.0322 - accuracy: 0.5161\n",
      "Epoch 112/125\n",
      "919/919 [==============================] - 193s 210ms/step - loss: 2.0144 - accuracy: 0.5196\n",
      "Epoch 113/125\n",
      "919/919 [==============================] - 192s 209ms/step - loss: 1.9945 - accuracy: 0.5241\n",
      "Epoch 114/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 1.9770 - accuracy: 0.5279\n",
      "Epoch 115/125\n",
      "919/919 [==============================] - 196s 213ms/step - loss: 1.9581 - accuracy: 0.5311\n",
      "Epoch 116/125\n",
      "919/919 [==============================] - 196s 213ms/step - loss: 1.9393 - accuracy: 0.5352\n",
      "Epoch 117/125\n",
      "919/919 [==============================] - 192s 209ms/step - loss: 1.9193 - accuracy: 0.5393\n",
      "Epoch 118/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 1.9036 - accuracy: 0.5418\n",
      "Epoch 119/125\n",
      "919/919 [==============================] - 191s 208ms/step - loss: 1.8912 - accuracy: 0.5440\n",
      "Epoch 120/125\n",
      "919/919 [==============================] - 193s 210ms/step - loss: 1.8722 - accuracy: 0.5480\n",
      "Epoch 121/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 1.8535 - accuracy: 0.5522\n",
      "Epoch 122/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 1.8340 - accuracy: 0.5555\n",
      "Epoch 123/125\n",
      "919/919 [==============================] - 192s 209ms/step - loss: 1.8200 - accuracy: 0.5583\n",
      "Epoch 124/125\n",
      "919/919 [==============================] - 193s 210ms/step - loss: 1.8032 - accuracy: 0.5625\n",
      "Epoch 125/125\n",
      "919/919 [==============================] - 195s 212ms/step - loss: 1.7918 - accuracy: 0.5652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size,seq_length) # 7235, 117533\n",
    "# aiming for accuracy to be little more than 50% not 100% since I want the model to understand the essence of text than to memorize.\n",
    "model.fit(X,y,epochs = 125, batch_size = 128)\n",
    "\n",
    "# saving the model for later use\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884fb48-a63d-42d4-91e8-0f34a16ee632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
